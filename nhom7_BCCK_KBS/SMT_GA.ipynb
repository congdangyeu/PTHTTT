{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUVGkzhxz3KM"
      },
      "source": [
        "#Phương pháp \n",
        "P(Target|Source) = P(S|T) * P(T) /P(S)\n",
        "=> rút gọn P(T|S) = P(S|T) * P(T)\n",
        "\n",
        "max P(T|S) = argmax(P(S|T) * P(T))\n",
        "Translation Model nó giải quyết cái P(S|T)\n",
        "Language Model P(T)\n",
        "Decoder (Genertic Algorithm) argmax "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lL6HIH5tjQD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg8Yxoe6tjly"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "id": "LAFv-qjEQ-1h",
        "outputId": "51ea9f68-b645-4d34-921b-c272321c9f88"
      },
      "outputs": [],
      "source": [
        "!pip install pyvi\n",
        "\n",
        "!pip install -U pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQVhKmP6P1-L"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from pyvi import ViTokenizer, ViPosTagger\n",
        "import pandas as pd\n",
        "import random \n",
        "import numpy as np\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeByL83K147w"
      },
      "source": [
        "#Preprocessing Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eZeTKb-P9Qi"
      },
      "outputs": [],
      "source": [
        "tokenized_stores = {'DataVI': [], 'DataEN': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfxCKCzU-h7B",
        "outputId": "df7b3aa6-044c-44d3-9ec7-758a61b687a1"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "tokenized_tag = defaultdict(list)\n",
        "print(type(tokenized_tag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHA57Jj7RPcV"
      },
      "outputs": [],
      "source": [
        "#tách câu cho data tiếng việt\n",
        "file_name = \"C:/Users/congv/Desktop/DataVI.txt\"\n",
        "load = open(file_name, encoding='utf-8')\n",
        "# tach cac cau trong data \n",
        "sentencesvi = load.read().split(\"\\n\")\n",
        "# for sentence in sentencesvi:\n",
        "#   token_store = sentence.split(\" \")\n",
        "#   tokenized_stores['DataVI'].append(token_store)\n",
        "# print(tokenized_stores['DataVI'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jeQSXkbRU5k"
      },
      "outputs": [],
      "source": [
        "tokenized_stores['DataVI'] = [ViTokenizer.tokenize(i).split() for i in sentencesvi]\n",
        "print(tokenized_stores['DataVI'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-c5bMKm-qV_"
      },
      "outputs": [],
      "source": [
        "#gắn tag loại từ cho các từ đầu vào\n",
        "for i in sentencesvi:\n",
        "  tag =  ViPosTagger.postagging(ViTokenizer.tokenize(i))\n",
        "  tokenized_tag['DataVI'].append(tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWqa1z9QRt11"
      },
      "outputs": [],
      "source": [
        "file_name = \"C:/Users/congv/Desktop/DataEN.txt\" \n",
        "load = open(file_name, encoding='utf-8')\n",
        "# tach cac cau trong data \n",
        "sentences = load.read().split(\"\\n\")\n",
        "# tach cac cau thanh cac tu \n",
        "# hien tai chi tach tu don mot chu cai \n",
        "for sentence in sentences:\n",
        "  token_store = sentence.split(\" \")\n",
        "  tokenized_stores['DataEN'].append(token_store)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAbRZPswDvJo",
        "outputId": "1f8c9387-8192-40f3-c136-562692f6e099"
      },
      "outputs": [],
      "source": [
        "print(tokenized_stores['DataVI'])\n",
        "print(tokenized_stores['DataEN'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xymMj8akiJjT",
        "outputId": "f5b8b6cf-9861-4e27-c931-9390fb82d2fd"
      },
      "outputs": [],
      "source": [
        "train_size = len(tokenized_stores['DataEN'])\n",
        "\n",
        "print(train_size)\n",
        "print(len(tokenized_stores['DataEN']))\n",
        "print(len(tokenized_stores['DataVI']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QJJ7XP6-KEm"
      },
      "outputs": [],
      "source": [
        "for index in range(train_size):\n",
        "  tag = tokenized_tag['DataVI'][index]\n",
        "  tmp = []\n",
        "  # tmp = array chứa các từ đã được ghép/tách\n",
        "\n",
        "  i=0\n",
        "  while i in range(len(tag[1])):\n",
        "\n",
        "      if(i<=len(tag[1])-3):\n",
        "          \n",
        "          L_Nc= tag[1][i]==\"L\" and (tag[1][i+1]==\"Nc\" or tag[1][i+1]==\"N\") and tag[1][i+2]==\"N\"\n",
        "          E_N= (tag[1][i]==\"E\" and tag[0][i]==\"của\") and tag[1][i+1]==\"N\" and (tag[1][i+2]==\"N\" or tag[1][i+2]==\"P\" or (tag[1][i+2]==\"V\" and tag[0][i+2]==\"ấy\"))\n",
        "          \n",
        "          if(L_Nc or E_N):\n",
        "              tmp.append(tag[0][i]+\" \"+tag[0][i+1]+\" \"+tag[0][i+2])\n",
        "              i=i+3\n",
        "              continue\n",
        "              \n",
        "      if(i<=len(tag[1])-2):\n",
        "          \n",
        "          Nc_N= tag[1][i]==\"Nc\" and tag[1][i+1]==\"N\" # ghép từ phân loại (determiner) với danh từ\n",
        "          N_N= tag[1][i]==\"N\" and tag[1][i+1]==\"N\" # ghép danh từ ghép\n",
        "          N_P = tag[1][i]==\"N\" and tag[1][i+1]==\"P\" # ghép danh từ ghép\n",
        "          N_A = (i==0 or (i>0 and (tag[1][i-1]!=\"Nc\" and tag[1][i-1]!=\"M\") ) ) and tag[1][i]==\"N\" and tag[1][i+1]==\"A\" # ghép danh tính từ\n",
        "          E_P = (tag[1][i]==\"E\" and tag[0][i]==\"của\") and tag[1][i+1]==\"P\" # ghép 'của' với một đại từ (của tôi,...)\n",
        "          E_N = (tag[1][i]==\"E\" and tag[0][i]==\"của\") and (tag[1][i+1]==\"N\" or tag[1][i+1]==\"Np\") # ghép 'của' với một danh từ (của họ, ...)\n",
        "          L_N = tag[1][i]==\"L\" and tag[1][i+1]==\"N\" # ghép 'những' với một danh từ\n",
        "          R_N = tag[1][i]==\"R\" and tag[1][i+1]==\"N\" # ghép 'hằng', 'ngày'\n",
        "          N_V = (tag[1][i]==\"N\" or tag[1][i]==\"Nc\") and (tag[1][i+1]==\"V\" and tag[0][i+1]==\"ấy\") #ghép 'anh'/'cô' 'ấy' thành một từ\n",
        "          N_E = (tag[1][i]==\"N\" and tag[0][i]==\"mùi\") and (tag[1][i+1]==\"E\" and tag[0][i+1]==\"của\") #ghép 'mùi', 'của' thành một từ\n",
        "          \n",
        "          if(Nc_N or N_N or N_P or N_A or N_E or E_P or E_N or L_N or R_N or N_V):\n",
        "              tmp.append(tag[0][i] + \" \" + tag[0][i+1])\n",
        "              i=i+2\n",
        "              continue\n",
        "      # if (i+1) > len(tag[1])-1:\n",
        "      tmp.append(tag[0][i])\n",
        "      i+=1\n",
        "      # print(str(tmp)+\" current index: \"+str(i))\n",
        "  tokenized_stores['DataVI'][index]=tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7sEjUMe_SwF",
        "outputId": "4dcb5ebe-c9cb-4287-dfd4-29bc3c588c02"
      },
      "outputs": [],
      "source": [
        "tokenized_stores['DataVI'][10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0UOgL75EVKC",
        "outputId": "f16caa86-b987-475f-95b5-a8ca20d9fbe8"
      },
      "outputs": [],
      "source": [
        "# tạo từ vựng từ các từ đã tách \n",
        "vn_words = {}\n",
        "en_words = {}\n",
        "# neu tu da ton tai trong tu dien thi khong them vao ma chi them so lan xuat hien\n",
        "# neu tu chua ton tai thi them vao\n",
        "for key in tokenized_stores:\n",
        "  if str(key)[4] ==\"V\":\n",
        "    for sentence in tokenized_stores[key]:\n",
        "      for word in sentence:\n",
        "        if word in vn_words:\n",
        "          vn_words[word] +=1  \n",
        "        else: \n",
        "          vn_words[word] = 1  \n",
        "  else:\n",
        "    for sentence in tokenized_stores[key]:\n",
        "      for word in sentence:\n",
        "        if word in en_words:\n",
        "          en_words[word] +=1\n",
        "        else: \n",
        "          en_words[word] = 1\n",
        "vn_vocab = len(vn_words)\n",
        "en_vocab = len(en_words)\n",
        "print(\"Number of english words:\", en_vocab)\n",
        "print(\"Number of vietnamese words:\", vn_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGOEuVhKrQCR"
      },
      "outputs": [],
      "source": [
        "for i in vn_words:\n",
        "  print(i,vn_words[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIr-EPw0Gkc-"
      },
      "source": [
        "# Translation Model \n",
        "  Với translation model thì mục tiêu của nó là tính toàn được xác suất của từ ở ngôn ngữ nguồn khi biết ngôn ngữ đích \n",
        "  Công thức P(S|T) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiDCYftFGgm3"
      },
      "outputs": [],
      "source": [
        "#tạo P(S|T)\n",
        "p = {}\n",
        "uniform = 1/(vn_vocab) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBsPShB39W0h",
        "outputId": "0642709f-3322-41fd-9597-4a5f090068ab"
      },
      "outputs": [],
      "source": [
        "n_iters = 0\n",
        "max_iters = 20\n",
        "\n",
        "fine_tune = 0\n",
        "has_converged = False\n",
        "\n",
        "while n_iters < max_iters and has_converged == False: \n",
        "    has_converged = True\n",
        "    max_change = -1\n",
        "\n",
        "    n_iters += 1\n",
        "    count = {}\n",
        "    total = {}\n",
        "    for index in range(train_size):\n",
        "        s_total = {}\n",
        "        for vn_word in tokenized_stores['DataVI'][index]:\n",
        "            s_total[vn_word] = 0\n",
        "            for en_word in tokenized_stores['DataEN'][index]:\n",
        "                if (vn_word, en_word) not in p:\n",
        "                    p[(vn_word, en_word)] = uniform\n",
        "                s_total[vn_word] += p[(vn_word, en_word)]\n",
        "\n",
        "        for vn_word in tokenized_stores['DataVI'][index]:\n",
        "            for en_word in tokenized_stores['DataEN'][index]:\n",
        "                if (vn_word, en_word) not in count:\n",
        "                    count[(vn_word, en_word)] = 0\n",
        "                count[(vn_word, en_word)] += (p[(vn_word, en_word)] / s_total[vn_word])\n",
        "\n",
        "                if en_word not in total:\n",
        "                    total[en_word] = 0\n",
        "                total[en_word] += (p[(vn_word, en_word)] / s_total[vn_word])\n",
        "\n",
        "    # estimating the probabilities\n",
        "    for index in range(train_size):\n",
        "        for en_word in tokenized_stores['DataEN'][index]:\n",
        "            for vn_word in tokenized_stores['DataVI'][index]:\n",
        "                if abs(p[(vn_word, en_word)] - count[(vn_word, en_word)] / total[en_word]) > 0.01:\n",
        "                    has_converged = False\n",
        "                    max_change = max(max_change, abs(p[(vn_word, en_word)] - count[(vn_word, en_word)] / total[en_word]))\n",
        "                p[(vn_word, en_word)] = count[(vn_word, en_word)] / total[en_word]\n",
        "\n",
        "    print(\"Iteration \" + str(n_iters) + \" Completed, Maximum Change: \" + str(max_change))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z71-0v5YJivy",
        "outputId": "0072ab10-1cd2-4989-8bc9-2c07a99f3861"
      },
      "outputs": [],
      "source": [
        "sorted_t = sorted(p.items(), key = lambda k:(k[1], k[0]), reverse = True)\n",
        "print(sorted_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsdHV1lp9zMe"
      },
      "outputs": [],
      "source": [
        "file = open(\"C:/Users/congv/Desktop/TM-SMTGA.pkl\",\"wb\")\n",
        "pickle.dump(p, file)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3VI770jPx0z"
      },
      "outputs": [],
      "source": [
        "model_name = \"C:/Users/congv/Desktop/TM-SMTGA.pkl\"\n",
        "pickle_in = open(model_name,\"rb\")\n",
        "p ={}\n",
        "p = pickle.load(pickle_in)\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZRC5ze31xGc"
      },
      "source": [
        "#Language Model\n",
        "Model được sử dụng là Bi-grams: Model sẽ tính xác suất các cặp từ thường xuyên xuất hiện liền nhau trong câu từ đó tính ra được xác suất của từ tiếp theo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JyNp6eA2BJM",
        "outputId": "0125fc2a-b273-4391-f80d-85fad898efbe"
      },
      "outputs": [],
      "source": [
        "unigrams = {}\n",
        "bigrams = {}\n",
        "# training on the train_set\n",
        "def model(dataset_size, dataset_name):\n",
        "    global bigrams\n",
        "    global unigrams\n",
        "    for index in range(dataset_size):\n",
        "        token_A = ''\n",
        "        for en_token in tokenized_stores[dataset_name][index]:\n",
        "            if en_token not in unigrams:\n",
        "                unigrams[en_token] = 1\n",
        "            else:\n",
        "                unigrams[en_token] += 1\n",
        "            \n",
        "            token_B = en_token\n",
        "            if (token_A, token_B) not in bigrams:\n",
        "                bigrams[(token_A, token_B)] = 1\n",
        "            else:\n",
        "                bigrams[(token_A, token_B)] += 1\n",
        "            token_A = token_B\n",
        "\n",
        "model(train_size, 'DataEN')\n",
        "\n",
        "bigram_count = len(bigrams)\n",
        "unigram_count = len(unigrams)\n",
        "print(\"Number of Unique Bigrams:\", bigram_count)\n",
        "print(\"Number of Unique Unigrams:\", unigram_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d7EGhlS2R97"
      },
      "outputs": [],
      "source": [
        "def find_translation(vn_token):\n",
        "    for element in sorted_t:\n",
        "        if element[0][0] == vn_token:\n",
        "            return element[0][1]\n",
        "    return \"\"\n",
        "def get_prob(seq):\n",
        "    # bigram language model với smoothing\n",
        "    if len(seq) < 2:\n",
        "        return 1\n",
        "    score = 1\n",
        "    token_A = ''\n",
        "    for hi_token in seq:\n",
        "        token_B = hi_token\n",
        "        if (token_A, token_B) not in bigrams:\n",
        "            if token_A==token_B:\n",
        "                score*=1e-15\n",
        "            else:\n",
        "                score *= 1e-10\n",
        "        else:\n",
        "            base_token_count = 0\n",
        "            if token_A in unigrams:\n",
        "                base_token_count = unigrams[token_A]\n",
        "            score *= (bigrams[(token_A, token_B)] + 1) / (base_token_count + unigram_count)\n",
        "        token_A = token_B\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCD2JCPV91Jw"
      },
      "source": [
        "#Genetic Algorithm Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm3NiuyVIRjg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def vietnamese_concatenator(source_sentence):\n",
        "  tmp=[]\n",
        "  i=0\n",
        "  # print(source_sentence[0], source_sentence[1])\n",
        "  while i in range(len(source_sentence[1])):\n",
        "      if(i<=len(source_sentence[1])-3):\n",
        "          \n",
        "          L_Nc= source_sentence[1][i]==\"L\" and (source_sentence[1][i+1]==\"Nc\" or source_sentence[1][i+1]==\"N\") and source_sentence[1][i+2]==\"N\"\n",
        "          E_N= (source_sentence[1][i]==\"E\" and source_sentence[0][i]==\"của\") and source_sentence[1][i+1]==\"N\" and (source_sentence[1][i+2]==\"N\" or source_sentence[1][i+2]==\"P\" or (source_sentence[1][i+2]==\"V\" and source_sentence[0][i+2]==\"ấy\"))\n",
        "          \n",
        "          if(L_Nc or E_N):\n",
        "              tmp.append(source_sentence[0][i]+\" \"+source_sentence[0][i+1]+\" \"+source_sentence[0][i+2])\n",
        "              i=i+3\n",
        "              continue\n",
        "              \n",
        "      if(i<=len(source_sentence[1])-2):\n",
        "          \n",
        "          Nc_N= source_sentence[1][i]==\"Nc\" and source_sentence[1][i+1]==\"N\" # ghép từ phân loại (determiner) với danh từ\n",
        "          N_N= source_sentence[1][i]==\"N\" and source_sentence[1][i+1]==\"N\" # ghép danh từ ghép\n",
        "          N_P = source_sentence[1][i]==\"N\" and source_sentence[1][i+1]==\"P\" # ghép danh từ ghép\n",
        "          N_A = (i==0 or (i>0 and (source_sentence[1][i-1]!=\"Nc\" and source_sentence[1][i-1]!=\"M\") ) ) and source_sentence[1][i]==\"N\" and source_sentence[1][i+1]==\"A\" # ghép danh tính từ\n",
        "          E_P = (source_sentence[1][i]==\"E\" and source_sentence[0][i]==\"của\") and source_sentence[1][i+1]==\"P\" # ghép 'của' với một đại từ (của tôi,...)\n",
        "          E_N = (source_sentence[1][i]==\"E\" and source_sentence[0][i]==\"của\") and (source_sentence[1][i+1]==\"N\" or source_sentence[1][i+1]==\"Np\") # ghép 'của' với một danh từ (của họ, ...)\n",
        "          L_N = source_sentence[1][i]==\"L\" and source_sentence[1][i+1]==\"N\" # ghép 'những' với một danh từ\n",
        "          R_N = source_sentence[1][i]==\"R\" and source_sentence[1][i+1]==\"N\" # ghép 'hằng', 'ngày'\n",
        "          N_V = (source_sentence[1][i]==\"N\" or source_sentence[1][i]==\"Nc\") and (source_sentence[1][i+1]==\"V\" and source_sentence[0][i+1]==\"ấy\") #ghép 'anh'/'cô' 'ấy' thành một từ\n",
        "          N_E = (source_sentence[1][i]==\"N\" and source_sentence[0][i]==\"mùi\") and (source_sentence[1][i+1]==\"E\" and source_sentence[0][i+1]==\"của\") #ghép 'mùi', 'của' thành một từ\n",
        "          \n",
        "          if(Nc_N or N_N or N_P or N_A or N_E or E_P or E_N or L_N or R_N or N_V):\n",
        "              tmp.append(source_sentence[0][i] + \" \" + source_sentence[0][i+1])\n",
        "              i=i+2\n",
        "              continue\n",
        "      tmp.append(source_sentence[0][i])\n",
        "      i+=1\n",
        "  return tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XHl7a8RXsx7"
      },
      "outputs": [],
      "source": [
        "#tìm từ thay thế cho các từ chưa có trong cơ sở dữ liệu bằng 2 phương pháp (xâu chung con dài nhất và các từ xuất hiện nhiều nhất)\n",
        "def LCSubStr(str1, str2, N, M):\n",
        " \n",
        "    LCSuff = [[0 for k in range(M+1)] for l in range(N+1)]\n",
        "    mx = 0\n",
        "    for i in range(N + 1):\n",
        "        for j in range(M + 1):\n",
        "            if (i == 0 or j == 0):\n",
        "                LCSuff[i][j] = 0\n",
        "            elif (str1[i-1] == str2[j-1]):\n",
        "                LCSuff[i][j] = LCSuff[i-1][j-1] + 1\n",
        "                if(mx<LCSuff[i][j]):\n",
        "                  mx = LCSuff[i][j]\n",
        "            else:\n",
        "                LCSuff[i][j] = 0\n",
        "    return mx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24nqMNtqJtGF"
      },
      "outputs": [],
      "source": [
        "def fix_word (source_text):\n",
        "  for i in range(len(source_text[0])):\n",
        "    if source_text[0][i] not in vn_words:\n",
        "      max_len =0\n",
        "      tmp_str = ''\n",
        "      for word in vn_words:\n",
        "        tmp = LCSubStr(word, source_text[0][i] , len(word), len(source_text[0][i]))\n",
        "        if(tmp>max_len):\n",
        "          max_len=tmp\n",
        "          tmp_str=word\n",
        "      if(max_len > 2): \n",
        "        source_text[0][i] = tmp_str\n",
        "  return source_text  \n",
        "  print(source_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9Y9vGTLdkdT"
      },
      "outputs": [],
      "source": [
        "# tìm các bản dịch cho các từ kề với từ không có trong cơ sở dữ liệu\n",
        "def replace_word(source_text, i):\n",
        "  tmp_near = []\n",
        "  if i == 0:\n",
        "    trans_word_2 = find_translation(source_text[0][i+1])\n",
        "    find_word_2 =  find_near(trans_word_2)\n",
        "    blank = \"\"\n",
        "    find_word_1 =  find_near(blank)\n",
        "    tmp_near.append(find_word_1)\n",
        "    tmp_near.append(find_word_2)\n",
        "  elif i == len(source_text[0])-1:\n",
        "    trans_word_2 = find_translation(source_text[0][i-1])\n",
        "    find_word_2 =  find_near(trans_word_2)  \n",
        "    tmp_near.append(find_word_2)\n",
        "  else:\n",
        "    trans_word_2 = find_translation(source_text[0][i+1])\n",
        "    find_word_2 =  find_near(trans_word_2)\n",
        "    trans_word_1 = find_translation(source_text[0][i-1])\n",
        "    find_word_1 =  find_near(trans_word_1)\n",
        "    tmp_near.append(find_word_1)\n",
        "    tmp_near.append(find_word_2)\n",
        "  return tmp_near\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu0W_IYOmEX7"
      },
      "outputs": [],
      "source": [
        "sort_bigrams = sorted(bigrams.items(), key = lambda k:(k[1], k[0]), reverse = True)\n",
        "def find_near(word):\n",
        "  tmp_word = []\n",
        "  count = 2\n",
        "  for token in sort_bigrams:\n",
        "    if(token[0][0]== word):\n",
        "      tmp_word.append(token[0][1])\n",
        "      count -=1\n",
        "      if count == 0: return tmp_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNBpqXfYIEoC"
      },
      "outputs": [],
      "source": [
        "#tìm 3 bản dịch cho từng từ trong câu\n",
        "#duyệt từng từ(cụm từ) đã được chuẩn hóa theo tri thức và tìm top 3 bản dịch có xác suất cao nhất  \n",
        "from collections import defaultdict\n",
        "def find_n_translation(source_tok):\n",
        "  fix_word(source_tok)\n",
        "  k = 0\n",
        "  trans_table = defaultdict(list)\n",
        "  for word in source_tok[0]:\n",
        "    k+=1\n",
        "    c = 3\n",
        "    if word not in vn_words:\n",
        "      replace = replace_word(source_text,k-1)\n",
        "      for i in replace[0]:\n",
        "        trans_table[word].append(i)\n",
        "        p[(word,i)] = 10e-5\n",
        "    else:\n",
        "      for i in sorted_t:\n",
        "        if(c<=0): break\n",
        "        if(i[0][0]==word):\n",
        "          trans_table[word].append(i[0][1])\n",
        "          c-=1\n",
        "  return trans_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jRPaxSjJpTw"
      },
      "outputs": [],
      "source": [
        "trans_table = find_n_translation(source_text)\n",
        "\n",
        "print(trans_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8EdFSx53cYF"
      },
      "outputs": [],
      "source": [
        "def MT_prob(source_text,seq):\n",
        "  sou_len = len(source_text[0])\n",
        "  tar_len = len(seq)\n",
        "  scores = 1\n",
        "  flag = {}\n",
        "  for i in range(sou_len):\n",
        "    for j in range(tar_len):\n",
        "      if seq[j] in trans_table[source_text[0][i]]:\n",
        "        if (source_text[0][i],seq[j]) not in flag:\n",
        "          scores *= p[(source_text[0][i],seq[j])]\n",
        "          flag[(source_text[0][i],seq[j])] = 1\n",
        "        else: scores *= 1e-15 \n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdbU9gwxEskC"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def fitness(member,source_text):\n",
        "  prob_lm = get_prob(member)\n",
        "  prob_tm = MT_prob(source_text,member)\n",
        "  score = math.log(prob_lm,10)*10 + math.log(prob_tm,10)\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkI5-wZUEw-C"
      },
      "outputs": [],
      "source": [
        "def Single_crossover(a,b):\n",
        "  A = list(a)\n",
        "  B = list(b) \n",
        "\n",
        "  length = len(a)\n",
        "  #tìm vị trí để hoán đổi\n",
        "  k = random.randint(0,length-1)\n",
        "  for i in range(k,len(a)):\n",
        "    A[i],B[i] = B[i],A[i]\n",
        "\n",
        "  return A,B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s_14QikE1A1"
      },
      "outputs": [],
      "source": [
        "def mutate(member, probability):\n",
        "  new_member = copy.deepcopy(member)\n",
        "  for i in range(1, len(new_member)):\n",
        "      if random.random() < probability:\n",
        "              location_1 = random.randint(0,len(new_member)-1)\n",
        "              location_2 = random.randint(0,len(new_member)-1)\n",
        "              new_member[location_1],new_member[location_2] = new_member[location_2], new_member[location_1] \n",
        "  return new_member"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUIfsh6ZE3pQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "def create_new_member(trans_table,source_text):\n",
        "  # xây dựng một thứ tự sắp xếp cho câu \n",
        "  source_len = len(source_text[0])\n",
        "  member = []\n",
        "  flag = {}\n",
        "  go =True\n",
        "  for i in source_text[0]:\n",
        "    for j in trans_table[i]:\n",
        "      flag[j] = 0\n",
        "  k=0\n",
        "  while go:\n",
        "      for i in source_text[0]:\n",
        "        k+=1\n",
        "        word = random.sample(trans_table[i],1)[0]\n",
        "        if flag[word] == 0:\n",
        "          rand_word = word\n",
        "          member.append(rand_word)\n",
        "          flag[word] = 1\n",
        "        else: \n",
        "          while(flag[word]!=0):\n",
        "            word = random.sample(trans_table[i],1)[0]\n",
        "          rand_word = word\n",
        "          member.append(rand_word)\n",
        "          flag[word] = 1\n",
        "        if k==source_len: go=False\n",
        "  return member"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJWpKgvJE6JU"
      },
      "outputs": [],
      "source": [
        "def create_first_population(trans_table,source_text):\n",
        "  population = []\n",
        "  for i in range(100):\n",
        "    member= create_new_member(trans_table,source_text) \n",
        "    population.append(member)\n",
        "  return population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4nIwwSQE87Q"
      },
      "outputs": [],
      "source": [
        "def scores_of_population(population,source_text):\n",
        "  scores = []\n",
        "  for i in range(len(population)):\n",
        "    scores.append([fitness(population[i],source_text)])\n",
        "  return scores  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pog9I2gBK68"
      },
      "outputs": [],
      "source": [
        "def rankSelect(population,source_text):\n",
        "  rank = keeper_gen(population, source_text)\n",
        "  i = random.randint(0,len(rank)/5)\n",
        "  return population[rank[i][0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIM15Sj7dEh5"
      },
      "outputs": [],
      "source": [
        "def keeper_gen(population, source_text):\n",
        "  list_gen = {}\n",
        "  for i in range(len(population)):\n",
        "    score = fitness(population[i],source_text)\n",
        "    list_gen[i]=score\n",
        "  list_gen = sorted(list_gen.items(), key = lambda k:(k[1], k[0]), reverse = True)\n",
        "  # list_gen = sorted(list_gen.items(), key=lambda item: item[1])\n",
        "  return list_gen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ojc2EpkFDNp"
      },
      "outputs": [],
      "source": [
        "#main of Genetic Algorithm\n",
        "import numpy as np\n",
        "import copy\n",
        "def main():\n",
        "  trans_table = find_n_translation(source_text)\n",
        "  #create the first population\n",
        "  population = create_first_population(trans_table,source_text)\n",
        "  best = []\n",
        "  for i in range(100):\n",
        "    print(\"Step:\",i)\n",
        "    new_population = []\n",
        "    #evaluate the fitness of current population\n",
        "    scores = scores_of_population(population,source_text)\n",
        "    best = population[np.argmax(scores)]\n",
        "    probability = fitness(best,source_text)               # chờ LM và TM\n",
        "    print(best)\n",
        "    print(probability)\n",
        "    if probability > -10:\n",
        "      break\n",
        "    #crossover\n",
        "    for j in range(15):\n",
        "      new_1, new_2 = Single_crossover(rankSelect(population,source_text),rankSelect(population,source_text))\n",
        "      new_population = new_population + [new_1,new_2]\n",
        "    #mutation\n",
        "    for i in range(len(new_population)):\n",
        "      new_population[i] = np.copy(mutate(new_population[i], 0.4))\n",
        "    new_population += [population[np.argmax(scores)]]\n",
        "    keepers = keeper_gen(population,source_text)\n",
        "    len_pop = len(new_population)\n",
        "    for j in range(100-len_pop):           \n",
        "      new_population +=[population[keepers[j][0]]]\n",
        "    population = copy.deepcopy(new_population)\n",
        "  print(\"Translation for: '\", input_text, \"'is\",best)\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IhFVavF8_76",
        "outputId": "bc1fb03d-09ef-4828-d615-eda46e89527d"
      },
      "outputs": [],
      "source": [
        "# Source sentence for translation\n",
        "# Qúa trình chuẩn hóa đầu vào người dùng nhập\n",
        "input_text = \"Tôi là một học sinh\"\n",
        "input_text = input_text.lower()  \n",
        "input_tag =  ViPosTagger.postagging(ViTokenizer.tokenize(input_text))\n",
        "source_text = [vietnamese_concatenator(input_tag)]\n",
        "print(source_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3rzdAm4KAel",
        "outputId": "004eacb8-7964-496b-e348-098fbabc4b66"
      },
      "outputs": [],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "08f4aa0acb1cdf7c53933ec40f4e697ee4f2d65f4b23006658cca62a0312aadf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
